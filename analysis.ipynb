{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86c641ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import linregress, zscore\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from plottable import Table\n",
    "from plottable import ColumnDefinition\n",
    "from plottable.plots import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2740eace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 5 files from xfer, 27 files from rec, and supplemental data\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "\n",
    "xferpath = Path(r\"C:\\Users\\User\\Documents\\cfb project\\data\\transferportal\\cleaned\")\n",
    "recpath = Path(r\"C:\\Users\\User\\Documents\\cfb project\\data\\recruiting\")\n",
    "xferfiles = {}\n",
    "recfiles = {}\n",
    "\n",
    "for file_path in xferpath.glob('*_cleaned.graphml'):\n",
    "    name = file_path.stem.replace('_cleaned', '')\n",
    "    G_x = nx.read_graphml(file_path)\n",
    "    xferfiles[name] = G_x\n",
    "\n",
    "for file_path in recpath.glob('*.graphml'):\n",
    "    name = file_path.stem\n",
    "    G_r = nx.read_graphml(file_path)\n",
    "    recfiles[name] = G_r\n",
    "\n",
    "nd_npv_ppa_df = pd.read_csv(r'C:\\Users\\User\\Documents\\cfb project\\data\\analysis\\nd_npv\\degree_analysis_ppa.csv')\n",
    "nd_npv_df = pd.read_csv(r'C:\\Users\\User\\Documents\\cfb project\\data\\analysis\\nd_npv\\degree_analysis_rating.csv')\n",
    "#rvi_gvo_df = pd.read_csv(r'')\n",
    "teamratings_df = pd.read_csv(r'C:\\Users\\User\\Documents\\cfb project\\data\\supplemental\\team_performance.csv')\n",
    "playerratings_df = pd.read_csv(r'C:\\Users\\User\\Documents\\cfb project\\data\\supplemental\\player_performance.csv', encoding='latin1')\n",
    "rosters_df = pd.read_csv(r'C:\\Users\\User\\Documents\\cfb project\\data\\supplemental\\rosters.csv', dtype={5: str})\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(xferfiles)} files from xfer, {len(recfiles)} files from rec, and supplemental data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5f20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files...\n",
      "\n",
      "Successfully created npv_df with 1591 records.\n",
      "Years covered: [np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "\n",
      "Sample Data (first 5 rows):\n",
      "   year          school  NPV_zscore  NPV_raw  NPV_assumed\n",
      "0  2021        Missouri       -2.48    -9.34        -9.34\n",
      "1  2021            UTSA        1.55     5.00         5.85\n",
      "2  2021  Arkansas State        2.70    10.18        10.18\n",
      "3  2021   Southern Miss        0.47     1.76         1.76\n",
      "4  2021           Miami       -0.23    -0.85        -0.85\n"
     ]
    }
   ],
   "source": [
    "# ND/NPV\n",
    "\n",
    "# 1. DEFINE CALCULATION LOGIC\n",
    "\n",
    "def nd_npv_rating(G):\n",
    "    \"\"\"\n",
    "    Calculate NPV for each school based on summing individual player ratings.\n",
    "    Includes uncertainty bounds for missing data.\n",
    "    \"\"\"\n",
    "    school_stats = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "\n",
    "        in_transfers = []\n",
    "        out_transfers = []\n",
    "        in_missing = 0\n",
    "        out_missing = 0\n",
    "        \n",
    "        # incoming\n",
    "        for pred in G.predecessors(node):\n",
    "            for key, data in G[pred][node].items():\n",
    "                rating = data.get('ratings', '')\n",
    "                try:\n",
    "                    # check if rating is a valid number\n",
    "                    if rating and str(rating) not in ['', '0.0', 'None', 'nan']:\n",
    "                        in_transfers.append(float(rating))\n",
    "                    else:\n",
    "                        in_missing += 1\n",
    "                except ValueError:\n",
    "                    in_missing += 1\n",
    "        \n",
    "        # outgoing\n",
    "        for succ in G.successors(node):\n",
    "            for key, data in G[node][succ].items():\n",
    "                rating = data.get('ratings', '')\n",
    "                try:\n",
    "                    if rating and str(rating) not in ['', '0.0', 'None', 'nan']:\n",
    "                        out_transfers.append(float(rating))\n",
    "                    else:\n",
    "                        out_missing += 1\n",
    "                except ValueError:\n",
    "                    out_missing += 1\n",
    "        \n",
    "        # calculate sums\n",
    "        in_sum = sum(in_transfers)\n",
    "        out_sum = sum(out_transfers)\n",
    "        npv = in_sum - out_sum\n",
    "        \n",
    "        # calculate uncertainty bounds\n",
    "        best_case_npv = (in_sum + in_missing * 0.95) - (out_sum + out_missing * 0.75)\n",
    "        worst_case_npv = (in_sum + in_missing * 0.75) - (out_sum + out_missing * 0.95)\n",
    "        \n",
    "        # assumed npv is the midpoint\n",
    "        assumed_npv = (best_case_npv + worst_case_npv) / 2\n",
    "        \n",
    "        # if no missing data, assumed is just raw npv\n",
    "        if (in_missing + out_missing) == 0:\n",
    "            assumed_npv = npv\n",
    "            \n",
    "        uncertainty = (best_case_npv - worst_case_npv) / 2\n",
    "        \n",
    "        conference = G.nodes[node].get('conference')\n",
    "        classification = G.nodes[node].get('classification')\n",
    "\n",
    "        school_stats[node] = {\n",
    "            'conference': conference,\n",
    "            'classification': classification,\n",
    "            'NPV_raw': round(npv, 2),\n",
    "            'NPV_assumed': round(assumed_npv, 2),\n",
    "            'in_wt': round(in_sum, 2),\n",
    "            'out_wt': round(out_sum, 2),\n",
    "            'net_degree': len(in_transfers) - len(out_transfers),\n",
    "            'in_count': len(in_transfers),\n",
    "            'out_count': len(out_transfers),\n",
    "            'in_missing': in_missing,\n",
    "            'out_missing': out_missing,\n",
    "            'missing': in_missing + out_missing,\n",
    "            'uncertainty': round(uncertainty, 2),\n",
    "            'NPV_high': round(best_case_npv, 2),\n",
    "            'NPV_low': round(worst_case_npv, 2)\n",
    "        }\n",
    "    \n",
    "    return school_stats\n",
    "\n",
    "# 2. EXECUTE AND BUILD DATAFRAME\n",
    "\n",
    "all_npv_records = []\n",
    "\n",
    "print(\"Processing files...\")\n",
    "\n",
    "for filename, G in xferfiles.items():\n",
    "    # extract year from filename\n",
    "    year_match = re.search(r'(\\d{4})', filename)\n",
    "    if year_match:\n",
    "        year = int(year_match.group(1))\n",
    "        \n",
    "        # calculate stats for this year\n",
    "        stats = nd_npv_rating(G)\n",
    "        \n",
    "        # flatten into list of dicts\n",
    "        for school, data in stats.items():\n",
    "            record = data.copy()\n",
    "            record['school'] = school\n",
    "            record['year'] = year\n",
    "            all_npv_records.append(record)\n",
    "    else:\n",
    "        print(f\"  Warning: Could not extract year from {filename}\")\n",
    "\n",
    "# create df\n",
    "npv_df = pd.DataFrame(all_npv_records)\n",
    "\n",
    "# 3. CALCULATE STANDARDIZED METRICS\n",
    "\n",
    "if not npv_df.empty:\n",
    "    # standardize school names\n",
    "    npv_df['school'] = npv_df['school'].astype(str).str.strip()\n",
    "    \n",
    "    # calculate z scores of assumed npv by specific year\n",
    "    npv_df['NPV_zscore'] = npv_df.groupby('year')['NPV_assumed'].transform(\n",
    "        lambda x: zscore(x, nan_policy='omit')\n",
    "    )\n",
    "    \n",
    "    # calculate percentiles\n",
    "    npv_df['NPV_percentile'] = npv_df.groupby('year')['NPV_assumed'].rank(pct=True) * 100\n",
    "    \n",
    "    # round to 2 decimal places\n",
    "    npv_df['NPV_zscore'] = npv_df['NPV_zscore'].round(2)\n",
    "    npv_df['NPV_percentile'] = npv_df['NPV_percentile'].round(2)\n",
    "\n",
    "    # reorder columns\n",
    "    cols = ['year', 'school', 'classification', 'conference', 'NPV_zscore', \n",
    "            'NPV_percentile', 'NPV_raw', 'NPV_assumed', \n",
    "            'net_degree', 'in_count', 'out_count', \n",
    "            'in_wt', 'out_wt', 'uncertainty', 'NPV_high', 'NPV_low',\n",
    "            'missing', 'in_missing', 'out_missing']\n",
    "            \n",
    "    # keep only columns that exist (in case no missing data found)\n",
    "    cols = [c for c in cols if c in npv_df.columns]\n",
    "    npv_df = npv_df[cols]\n",
    "\n",
    "    print(f\"\\nSuccessfully created npv_df with {len(npv_df)} records.\")\n",
    "    print(f\"Years covered: {sorted(npv_df['year'].unique())}\")\n",
    "    print(\"\\nSample Data (first 5 rows):\")\n",
    "    print(npv_df[['year', 'school', 'NPV_zscore', 'NPV_raw', 'NPV_assumed']].head().to_string())\n",
    "else:\n",
    "    print(\"Error: No data records generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95445df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CALCULATING COMPREHENSIVE TALENT METRICS\n",
      "======================================================================\n",
      "\n",
      "Step 1: Processing recruiting data & extracting fallback metadata...\n",
      "  2000: Found 119 recruits\n",
      "  2001: Found 129 recruits\n",
      "  2002: Found 1151 recruits\n",
      "  2003: Found 1328 recruits\n",
      "  2004: Found 1370 recruits\n",
      "  2005: Found 1428 recruits\n",
      "  2006: Found 1735 recruits\n",
      "  2007: Found 1725 recruits\n",
      "  2008: Found 1861 recruits\n",
      "  2009: Found 1791 recruits\n",
      "  2010: Found 2149 recruits\n",
      "  2011: Found 2013 recruits\n",
      "  2012: Found 2430 recruits\n",
      "  2013: Found 2358 recruits\n",
      "  2014: Found 2454 recruits\n",
      "  2015: Found 2343 recruits\n",
      "  2016: Found 2653 recruits\n",
      "  2017: Found 2778 recruits\n",
      "  2018: Found 3081 recruits\n",
      "  2019: Found 2248 recruits\n",
      "  2020: Found 2346 recruits\n",
      "  2021: Found 1803 recruits\n",
      "  2022: Found 1585 recruits\n",
      "  2023: Found 1778 recruits\n",
      "  2024: Found 1936 recruits\n",
      "  2025: Found 1971 recruits\n",
      "  Calculating recruiting Z-scores (Player Level)...\n",
      "\n",
      "Step 2: Building player tracking system...\n",
      "\n",
      "Step 3: Identifying graduating players...\n",
      "  Identified 129736 graduating players\n",
      "\n",
      "Step 4: Aggregating RVI and GVO...\n",
      "\n",
      "Step 5: Preparing NPV data from nd_npv_df...\n",
      "  Loaded NPV data: 1591 records\n",
      "\n",
      "Step 6: Creating Master DataFrame...\n",
      "\n",
      "Step 7: Calculating TD and PCR...\n",
      "\n",
      "Step 8: Finalizing output...\n",
      "\n",
      "✓ Saved to: C:\\Users\\User\\Documents\\cfb project\\data\\analysis\\comprehensive_talent_metrics.csv\n",
      "  Final Data Shape: (5025, 26)\n",
      "\n",
      "Top 5 Schools by TD_raw in 2024:\n",
      " year            school  TD_raw  RVI_zscore  NPV_zscore  GVO_zscore\n",
      " 2024     Hudson Valley   11.65        1.11        0.00       -0.82\n",
      " 2024 Stephen F. Austin    5.85       -0.48        2.42       -0.61\n",
      " 2024          Delaware    5.00       -0.35        1.40       -0.70\n",
      " 2024  Garden City C.C.    4.19       -0.74        1.20       -0.82\n",
      " 2024    Incarnate Word    4.14       -0.74        1.98       -0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_23392\\3419456930.py:323: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df[numeric_cols] = final_df[numeric_cols].round(2)\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE TALENT METRICS CALCULATION\n",
    "# RVI, GVO, NPV, TD, PCR\n",
    "\n",
    "# STEP 1: EXTRACT RECRUITING DATA & METADATA\n",
    "\n",
    "print(\"\\nStep 1: Processing recruiting data & extracting fallback metadata...\")\n",
    "\n",
    "all_recruits = []\n",
    "school_metadata_fallback = {} # (year, school) -> {conf, class} for years/teams not in npv df\n",
    "\n",
    "for year in range(2000, 2026):\n",
    "    graph_name = f\"recruiting_network_{year}\"\n",
    "    \n",
    "    if graph_name not in recfiles:\n",
    "        # print(f\"  Warning: {graph_name} not found, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    G = recfiles[graph_name]\n",
    "    count_for_year = 0\n",
    "    \n",
    "    # 1. iterate edges to get recruits and metadata\n",
    "    for u, v, key, data in G.edges(data=True, keys=True):\n",
    "        school_node = None\n",
    "        \n",
    "        # identify school node\n",
    "        if G.nodes[v].get('type') == 'School':\n",
    "            school_node = v\n",
    "        elif G.nodes[u].get('type') == 'School':\n",
    "            school_node = u\n",
    "            \n",
    "        if school_node:\n",
    "            school_name = str(school_node).strip()\n",
    "            \n",
    "            # extract metadata (as fallback for non-NPV years)\n",
    "            if (year, school_name) not in school_metadata_fallback:\n",
    "                node_attrs = G.nodes[school_node]\n",
    "                school_metadata_fallback[(year, school_name)] = {\n",
    "                    'classification': node_attrs.get('classification', 'Unknown'),\n",
    "                    'conference': node_attrs.get('conference', 'Unknown')\n",
    "                }\n",
    "\n",
    "            # 3. extract recruit data\n",
    "            rating = data.get('rating', None)\n",
    "            stars = data.get('stars', None)\n",
    "            \n",
    "            if rating is not None:\n",
    "                try:\n",
    "                    rating_val = float(rating)\n",
    "                    if rating_val > 0:\n",
    "                        all_recruits.append({\n",
    "                            'year': year,\n",
    "                            'school': school_name,\n",
    "                            'player_name': data.get('player'),\n",
    "                            'recruit_id': data.get('id'),\n",
    "                            'athlete_id': data.get('athlete_id'),\n",
    "                            'rating': rating_val,\n",
    "                            'stars': int(stars) if stars else None,\n",
    "                            'position': data.get('position')\n",
    "                        })\n",
    "                        count_for_year += 1\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "\n",
    "    print(f\"  {year}: Found {count_for_year} recruits\")\n",
    "\n",
    "df_recruits = pd.DataFrame(all_recruits)\n",
    "\n",
    "# metrics for recruits\n",
    "if not df_recruits.empty:\n",
    "    df_recruits['school'] = df_recruits['school'].astype(str).str.strip()\n",
    "    print(\"  Calculating recruiting Z-scores (Player Level)...\")\n",
    "    df_recruits['rating_percentile'] = df_recruits.groupby('year')['rating'].rank(pct=True) * 100\n",
    "    df_recruits['rating_zscore'] = df_recruits.groupby('year')['rating'].transform(lambda x: zscore(x, nan_policy='omit'))\n",
    "else:\n",
    "    print(\"CRITICAL WARNING: No recruits found.\")\n",
    "\n",
    "# STEP 2: BUILD PLAYER TRACKING (For GVO)\n",
    "\n",
    "print(\"\\nStep 2: Building player tracking system...\")\n",
    "player_recruit_info = {}\n",
    "for _, row in df_recruits.iterrows():\n",
    "    aid = str(row['athlete_id']) if pd.notna(row['athlete_id']) else None\n",
    "    if aid and aid != 'Unknown':\n",
    "        player_recruit_info[aid] = {\n",
    "            'rating': row['rating'],\n",
    "            'rating_zscore': row['rating_zscore'],\n",
    "            'rating_percentile': row['rating_percentile']\n",
    "        }\n",
    "\n",
    "# STEP 3: IDENTIFY GRADUATING PLAYERS (GVO)\n",
    "\n",
    "print(\"\\nStep 3: Identifying graduating players...\")\n",
    "rosters_df['player_id'] = rosters_df['player_id'].astype(str)\n",
    "rosters_df['year'] = rosters_df['year'].astype(int)\n",
    "\n",
    "graduating_players = []\n",
    "for year in range(2000, 2025):\n",
    "    curr = rosters_df[rosters_df['year'] == year]\n",
    "    nxt = rosters_df[rosters_df['year'] == year + 1]\n",
    "    next_ids = set(nxt['player_id'].unique())\n",
    "    \n",
    "    for _, player in curr.iterrows():\n",
    "        if player['player_id'] not in next_ids:\n",
    "            rec_info = player_recruit_info.get(str(player['player_id']), None)\n",
    "            graduating_players.append({\n",
    "                'grad_year': year,\n",
    "                'school': str(player['team']).strip(),\n",
    "                'rating': rec_info['rating'] if rec_info else None,\n",
    "                'rating_zscore': rec_info['rating_zscore'] if rec_info else None,\n",
    "                'rating_percentile': rec_info['rating_percentile'] if rec_info else None\n",
    "            })\n",
    "\n",
    "df_graduating = pd.DataFrame(graduating_players)\n",
    "print(f\"  Identified {len(df_graduating)} graduating players\")\n",
    "\n",
    "# STEP 4: AGGREGATE RVI & GVO\n",
    "\n",
    "print(\"\\nStep 4: Aggregating RVI and GVO...\")\n",
    "\n",
    "# RVI\n",
    "rvi_agg = df_recruits.groupby(['year', 'school']).agg({\n",
    "    'rating_zscore': 'sum', 'rating_percentile': 'sum', 'rating': 'sum', 'recruit_id': 'count'\n",
    "}).rename(columns={\n",
    "    'rating_zscore': 'RVI_zscore', 'rating_percentile': 'RVI_percentile', \n",
    "    'rating': 'RVI_raw', 'recruit_id': 'RVI_count'\n",
    "}).reset_index()\n",
    "\n",
    "# GVO\n",
    "gvo_agg = df_graduating[df_graduating['rating'].notna()].groupby(['grad_year', 'school']).agg({\n",
    "    'rating_zscore': 'sum', 'rating_percentile': 'sum', 'rating': 'sum', 'rating': 'count' # count gives player count\n",
    "}).rename(columns={\n",
    "    'rating_zscore': 'GVO_zscore', 'rating_percentile': 'GVO_percentile', \n",
    "    'rating': 'GVO_count' # re-mapped below for raw vs count\n",
    "}).reset_index().rename(columns={'grad_year': 'year'})\n",
    "\n",
    "# Fix GVO Raw/Count (the agg above overwrote rating sum with count, let's do it cleanly)\n",
    "gvo_agg = df_graduating[df_graduating['rating'].notna()].groupby(['grad_year', 'school']).agg({\n",
    "    'rating_zscore': 'sum', 'rating_percentile': 'sum', 'rating': 'sum'\n",
    "}).rename(columns={'rating_zscore': 'GVO_zscore', 'rating_percentile': 'GVO_percentile', 'rating': 'GVO_raw'}).reset_index().rename(columns={'grad_year': 'year'})\n",
    "\n",
    "gvo_counts = df_graduating.groupby(['grad_year', 'school']).size().reset_index(name='GVO_total_count').rename(columns={'grad_year': 'year'})\n",
    "# rated count\n",
    "gvo_rated_counts = df_graduating[df_graduating['rating'].notna()].groupby(['grad_year', 'school']).size().reset_index(name='GVO_count').rename(columns={'grad_year': 'year'})\n",
    "\n",
    "# STEP 5: PREPARE NPV\n",
    "\n",
    "print(\"\\nStep 5: Preparing NPV data from nd_npv_df...\")\n",
    "\n",
    "if 'nd_npv_df' in locals():\n",
    "    npv_data = nd_npv_df.copy()\n",
    "    \n",
    "    # 1. standardize school\n",
    "    if 'school' not in npv_data.columns and 'team' in npv_data.columns:\n",
    "        npv_data = npv_data.rename(columns={'team': 'school'})\n",
    "    npv_data['school'] = npv_data['school'].astype(str).str.strip()\n",
    "    \n",
    "    # 4. calculate z score if not present\n",
    "    if 'NPV_zscore' not in npv_data.columns and 'NPV_assumed' in npv_data.columns:\n",
    "        print(\"  Calculating NPV z-scores...\")\n",
    "        npv_data['NPV_zscore'] = npv_data.groupby('year')['NPV_assumed'].transform(lambda x: zscore(x, nan_policy='omit'))\n",
    "    \n",
    "    print(f\"  Loaded NPV data: {len(npv_data)} records\")\n",
    "else:\n",
    "    print(\"  CRITICAL ERROR: nd_npv_df not found.\")\n",
    "    npv_data = pd.DataFrame(columns=['year', 'school'])\n",
    "\n",
    "# STEP 6: MASTER MERGE\n",
    "\n",
    "print(\"\\nStep 6: Creating Master DataFrame...\")\n",
    "\n",
    "# 1. create spine from all sources\n",
    "sources = [rvi_agg[['year', 'school']], gvo_agg[['year', 'school']], npv_data[['year', 'school']]]\n",
    "# add fallback metadata keys to spine to ensure pre-2021 teams exist\n",
    "fb_keys = [{'year': k[0], 'school': k[1]} for k in school_metadata_fallback.keys()]\n",
    "if fb_keys: sources.append(pd.DataFrame(fb_keys))\n",
    "\n",
    "master = pd.concat([df for df in sources if not df.empty]).drop_duplicates().sort_values(['year', 'school'])\n",
    "\n",
    "# 2. merge data\n",
    "merged = master.merge(rvi_agg, on=['year', 'school'], how='left')\n",
    "merged = merged.merge(gvo_agg, on=['year', 'school'], how='left')\n",
    "merged = merged.merge(gvo_counts, on=['year', 'school'], how='left')\n",
    "merged = merged.merge(gvo_rated_counts, on=['year', 'school'], how='left')\n",
    "merged = merged.merge(npv_data, on=['year', 'school'], how='left')\n",
    "\n",
    "# 3. handle metadata (conference/classification)\n",
    "#    priority 1: use what's in npv_data\n",
    "#    priority 2: se fallback from recruiting graphs\n",
    "if 'conference' not in merged.columns:\n",
    "    # use fallback dict\n",
    "    def get_meta(row, col):\n",
    "        key = (row['year'], row['school'])\n",
    "        if key in school_metadata_fallback:\n",
    "            return school_metadata_fallback[key].get(col, 'Unknown')\n",
    "        return 'Unknown'\n",
    "        \n",
    "    merged['conference'] = merged.apply(lambda x: get_meta(x, 'conference'), axis=1)\n",
    "    merged['classification'] = merged.apply(lambda x: get_meta(x, 'classification'), axis=1)\n",
    "else:\n",
    "    # if merged from npv, fill gaps using fallback for non-npv years\n",
    "    # because npv data likely only exists for 2021\n",
    "    print(\"  Using existing metadata columns, filling gaps...\")\n",
    "    for idx, row in merged[merged['conference'].isna()].iterrows():\n",
    "        key = (row['year'], row['school'])\n",
    "        if key in school_metadata_fallback:\n",
    "            merged.at[idx, 'conference'] = school_metadata_fallback[key].get('conference', 'Unknown')\n",
    "            merged.at[idx, 'classification'] = school_metadata_fallback[key].get('classification', 'Unknown')\n",
    "\n",
    "# fill metric NaNs with 0 for calculation\n",
    "metric_cols = [c for c in merged.columns if any(x in c for x in ['RVI', 'GVO', 'NPV'])]\n",
    "merged[metric_cols] = merged[metric_cols].fillna(0)\n",
    "\n",
    "# STEP 7: CALCULATE COMPOSITE METRICS\n",
    "\n",
    "print(\"\\nStep 7: Calculating TD and PCR...\")\n",
    "\n",
    "# 1. calculate team-level z scores for rvi and gvo\n",
    "merged['RVI_team_zscore'] = merged.groupby('year')['RVI_raw'].transform(\n",
    "    lambda x: zscore(x, nan_policy='omit')\n",
    ")\n",
    "\n",
    "merged['GVO_team_zscore'] = merged.groupby('year')['GVO_raw'].transform(\n",
    "    lambda x: zscore(x, nan_policy='omit')\n",
    ")\n",
    "\n",
    "# 2. calculate talent delta\n",
    "merged['TD_zscore'] = (\n",
    "    merged['RVI_team_zscore'].fillna(0) + \n",
    "    merged['NPV_zscore'].fillna(0) - \n",
    "    merged['GVO_team_zscore'].fillna(0)\n",
    ")\n",
    "\n",
    "merged['TD_raw'] = merged['RVI_raw'] + merged['NPV_raw'] - merged['GVO_raw']\n",
    "merged['TD_count'] = merged['RVI_count'] + merged['NPV_net_count'] - merged['GVO_count']\n",
    "\n",
    "# 3. calculate portal contribution ratio (pcr)\n",
    "merged['PCR_raw'] = np.where(merged['RVI_raw'] != 0, merged['NPV_raw'] / merged['RVI_raw'], 0)\n",
    "merged['PCR_zscore'] = np.where(\n",
    "    merged['RVI_team_zscore'] != 0, \n",
    "    merged['NPV_zscore'] / merged['RVI_team_zscore'], \n",
    "    0\n",
    ")\n",
    "\n",
    "# 4. roster churn\n",
    "roster_sizes = rosters_df.groupby(['year', 'team']).size().reset_index(name='roster_size')\n",
    "merged = merged.merge(roster_sizes, left_on=['year', 'school'], right_on=['year', 'team'], how='left')\n",
    "merged = merged.drop(columns=['team'])\n",
    "\n",
    "merged['new_players'] = merged['RVI_count'] + merged['NPV_in_count']\n",
    "merged['roster_churn_pct'] = np.where(\n",
    "    merged['roster_size'] > 0,\n",
    "    (merged['new_players'] / merged['roster_size']) * 100,\n",
    "    0\n",
    ")\n",
    "\n",
    "# 5. calculate team-level percentiles\n",
    "#    this replaces the \"sum of player percentiles\" with \"rank of team total\"\n",
    "merged['RVI_percentile'] = merged.groupby('year')['RVI_raw'].rank(pct=True) * 100\n",
    "merged['GVO_percentile'] = merged.groupby('year')['GVO_raw'].rank(pct=True) * 100\n",
    "\n",
    "# STEP 8: FINALIZE AND SAVE\n",
    "\n",
    "print(\"\\nStep 8: Finalizing output...\")\n",
    "\n",
    "# 1. Replace Player-Sum Z-Scores with Team-Level Z-Scores\n",
    "#    This effectively swaps the metric in the final output\n",
    "merged['RVI_zscore'] = merged['RVI_team_zscore'].fillna(0)\n",
    "merged['GVO_zscore'] = merged['GVO_team_zscore'].fillna(0)\n",
    "\n",
    "# 2. define strict column order\n",
    "column_order = [\n",
    "    # metadata\n",
    "    'year', 'school', 'classification', 'conference',\n",
    "    \n",
    "    # composite metrics\n",
    "    'TD_zscore', 'TD_raw', 'TD_count',\n",
    "    'PCR_zscore', 'PCR_raw',\n",
    "    \n",
    "    # rvi\n",
    "    'RVI_zscore', 'RVI_percentile', 'RVI_raw', 'RVI_count',\n",
    "    \n",
    "    # gvo\n",
    "    'GVO_zscore', 'GVO_percentile', 'GVO_raw', 'GVO_count', 'GVO_total_count',\n",
    "    \n",
    "    # npv\n",
    "    'NPV_zscore', 'NPV_percentile', 'NPV_raw', 'NPV_assumed', 'NPV_net_count',\n",
    "    \n",
    "    # roster context\n",
    "    'roster_size', 'new_players', 'roster_churn_pct'\n",
    "]\n",
    "\n",
    "# 3. filter and round\n",
    "# keep only the columns defined above that exist in the dataframe\n",
    "final_columns = [col for col in column_order if col in merged.columns]\n",
    "final_df = merged[final_columns]\n",
    "\n",
    "# round numeric columns\n",
    "numeric_cols = final_df.select_dtypes(include=[np.number]).columns\n",
    "final_df[numeric_cols] = final_df[numeric_cols].round(2)\n",
    "\n",
    "# 4. save\n",
    "output_path = Path(r\"C:\\Users\\User\\Documents\\cfb project\\data\\analysis\") / \"comprehensive_talent_metrics.csv\"\n",
    "try:\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✓ Saved to: {output_path}\")\n",
    "    print(f\"  Final Data Shape: {final_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error saving file: {e}\")\n",
    "\n",
    "# sample output\n",
    "print(\"\\nTop 5 Schools by TD_raw in 2024:\")\n",
    "sample = final_df[final_df['year'] == 2024].nlargest(5, 'TD_raw')[\n",
    "    ['year', 'school', 'TD_raw', 'RVI_zscore', 'NPV_zscore', 'GVO_zscore']\n",
    "]\n",
    "print(sample.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2bb266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPV by rating successful\n"
     ]
    }
   ],
   "source": [
    "# save results (1 for rating, 2 for ppa, 0 for neither)\n",
    "switch = 0\n",
    "if switch == 1: # prevent accidental overwrite\n",
    "    output_path = Path(r\"C:\\Users\\User\\Documents\\cfb project\\data\\analysis\\nd_npv\") / f\"degree_analysis_rating.csv\"\n",
    "    npv_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print('NPV by rating successful')\n",
    "\n",
    "if switch == 2:\n",
    "    output_path = Path(r\"C:\\Users\\User\\Documents\\cfb project\\data\\analysis\\nd_npv\") / f\"degree_analysis_ppa.csv\"\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print('NPV by PPA successful')\n",
    "\n",
    "if switch == 0:\n",
    "    print(\"Switch is off; operation skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b08d2a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'npv_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# distributions and summary stats\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ND/NPV SUMMARY STATS\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ---\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# ND VS NPV SCATTER\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mnpv_df\u001b[49m.plot.scatter(y=\u001b[33m'\u001b[39m\u001b[33mNPV_assumed\u001b[39m\u001b[33m'\u001b[39m, x=\u001b[33m'\u001b[39m\u001b[33mnet_degree\u001b[39m\u001b[33m'\u001b[39m, grid=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m filtered = npv_df[npv_df[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m2023\u001b[39m]\n\u001b[32m     10\u001b[39m filtered.plot.scatter(y=\u001b[33m'\u001b[39m\u001b[33mNPV_assumed\u001b[39m\u001b[33m'\u001b[39m, x=\u001b[33m'\u001b[39m\u001b[33mnet_degree\u001b[39m\u001b[33m'\u001b[39m, grid=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'npv_df' is not defined"
     ]
    }
   ],
   "source": [
    "# distributions and summary stats\n",
    "\n",
    "# ND/NPV SUMMARY STATS\n",
    "# ---\n",
    "\n",
    "# ND VS NPV SCATTER\n",
    "\n",
    "npv_df.plot.scatter(y='NPV_assumed', x='net_degree', grid=True)\n",
    "filtered = npv_df[npv_df['year'] == 2023]\n",
    "filtered.plot.scatter(y='NPV_assumed', x='net_degree', grid=True)\n",
    "\n",
    "# AGG METRICS BY SCHOOL\n",
    "\n",
    "# 1. Prepare your data\n",
    "school_summary_npv = npv_df.groupby('school')['net_degree'].sum().reset_index().sort_values('net_degree', ascending=False)\n",
    "\n",
    "# 2. Create explicit copies for Top and Bottom to avoid SettingWithCopy warnings\n",
    "#    and manipulate their indices independently.\n",
    "\n",
    "# --- Top 15 (Highest Net Degree) ---\n",
    "df_top = school_summary_npv.head(15).copy()\n",
    "df_top.reset_index(drop=True, inplace=True)  # Reset to 0, 1, 2...\n",
    "df_top.index = df_top.index + 1             # Shift to 1, 2, 3...\n",
    "df_top.index.name = 'Rank'                  # Set the header label\n",
    "\n",
    "# --- Bottom 15 (Lowest Net Degree) ---\n",
    "# Your logic was correct: tail(15) gets the bottom, sort_values(asc=True) puts the lowest first.\n",
    "df_bottom = school_summary_npv.tail(15).sort_values('net_degree', ascending=True).copy()\n",
    "df_bottom.reset_index(drop=True, inplace=True) # Reset to 0, 1, 2...\n",
    "df_bottom.index = df_bottom.index + 1          # Shift to 1, 2, 3...\n",
    "df_bottom.index.name = 'Rank'                  # Set the header label\n",
    "\n",
    "# 3. Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6)) # Increased width slightly for readability\n",
    "plt.suptitle('Aggregate Net Portal Gain by School (Highest and Lowest)', fontsize=16)\n",
    "\n",
    "column_definitions = [ColumnDefinition(name='school', # name of the column to change\n",
    "                                       title='School', # new title for the column\n",
    "                                       textprops={\"ha\": \"left\"}),\n",
    "                      ColumnDefinition(name='net_degree',\n",
    "                                       title='Value',\n",
    "                                       textprops={\"ha\": \"right\"}),\n",
    "                      ColumnDefinition(name='Rank',\n",
    "                                       textprops={'ha': 'right'})\n",
    "                                      ]\n",
    "\n",
    "# Create the tables\n",
    "tab1 = Table(df_top, ax=axes[0], column_definitions=column_definitions)\n",
    "tab2 = Table(df_bottom, ax=axes[1], column_definitions=column_definitions)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# AGG TEAM RATINGS BY SCHOOL FOR SAME YEARS\n",
    "\n",
    "#teamratings_df['year'] = teamratings_df['season'].apply(lambda x: int(x.split('-')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network analysis\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
